{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6462ec8f-8d68-4052-9f0a-6b05ca05f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file C:\\Users\\User\\Downloads\\laugh.mp4, 1244160 bytes wanted but 0 bytes read,at frame 121/124, at time 4.84/4.95 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file C:\\Users\\User\\Downloads\\laugh.mp4, 1244160 bytes wanted but 0 bytes read,at frame 122/124, at time 4.88/4.95 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file C:\\Users\\User\\Downloads\\laugh.mp4, 1244160 bytes wanted but 0 bytes read,at frame 123/124, at time 4.92/4.95 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Moviepy - Building video output_laugh.mp4.\n",
      "MoviePy - Writing audio in output_laughTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_laugh.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_laugh.mp4\n"
     ]
    }
   ],
   "source": [
    "#I got clear output in saved output\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "# Load the trained model and label encoder\n",
    "model = load_model('voice_emotion_recognition.h5')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Define emotions and initialize trends\n",
    "emotions = ['anger', 'calm', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotion_trends = {emotion: [] for emotion in emotions}\n",
    "\n",
    "# Function to extract features from an audio chunk\n",
    "def extract_features(audio_chunk, rate):\n",
    "    mfccs = librosa.feature.mfcc(y=audio_chunk, sr=rate, n_mfcc=40)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs\n",
    "\n",
    "# Function to predict emotion from audio\n",
    "def predict_audio_emotion(audio_chunk, rate):\n",
    "    features = extract_features(audio_chunk, rate)\n",
    "    features = np.reshape(features, (1, -1, 1))\n",
    "    prediction = model.predict(features)\n",
    "    label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "    return label[0], prediction[0]\n",
    "\n",
    "# Function to generate a simulated alpha wave signal (EEG-like waveform)\n",
    "def generate_alpha_wave(length, amplitude=10, frequency=10):\n",
    "    t = np.linspace(0, 1, length)\n",
    "    alpha_wave = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "    return alpha_wave\n",
    "\n",
    "# Function to draw the dashboard with EEG-like waveform\n",
    "def draw_dashboard(frame, emotion_probs, emotion_trends):\n",
    "    # Dashboard settings\n",
    "    dashboard_width = 800\n",
    "    dashboard_height = frame.shape[0]\n",
    "    dashboard = np.ones((dashboard_height, dashboard_width, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "    # Bar settings\n",
    "    bar_height = dashboard_height // 8\n",
    "    bar_width = 300\n",
    "    wave_width = 300\n",
    "    shadow_height = 20  # Height of the shaded region\n",
    "    gap = 10  # Gap between shaded region and the bottom of the rectangle\n",
    "    vertical_padding = 10  # Padding to prevent waves from touching edges\n",
    "\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        # Draw emotion labels\n",
    "        cv2.putText(dashboard, emotion, (10, (i + 1) * bar_height - vertical_padding), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "        # Draw emotion probability bars\n",
    "        cv2.rectangle(dashboard, (110, i * bar_height), (110 + bar_width, (i + 1) * bar_height - vertical_padding), (0, 0, 0), 2)\n",
    "        cv2.rectangle(dashboard, (110, i * bar_height), \n",
    "                      (110 + int(bar_width * emotion_probs[i]), (i + 1) * bar_height - vertical_padding), \n",
    "                      (0, 255, 0), -1)  # Green color for bars\n",
    "\n",
    "        # Draw emotion trend waveforms centered vertically in the box\n",
    "        wave_x = 450\n",
    "        wave_y_start = i * bar_height + vertical_padding\n",
    "        wave_y_end = (i + 1) * bar_height - vertical_padding\n",
    "\n",
    "        # Calculate the center of the rectangle\n",
    "        wave_y_center = (wave_y_start + wave_y_end) // 2\n",
    "\n",
    "        # Draw the rectangle for the waveform\n",
    "        cv2.rectangle(dashboard, (wave_x, i * bar_height), (wave_x + wave_width, (i + 1) * bar_height - vertical_padding), (0, 0, 0), 2)\n",
    "\n",
    "        # Normalize trend data\n",
    "        if len(emotion_trends[emotion]) > 1:\n",
    "            # Add the new data point\n",
    "            emotion_trends[emotion].append(emotion_probs[i])\n",
    "            if len(emotion_trends[emotion]) > wave_width:\n",
    "                emotion_trends[emotion].pop(0)\n",
    "\n",
    "            norm_trend = (np.array(emotion_trends[emotion]) - np.min(emotion_trends[emotion])) / (np.max(emotion_trends[emotion]) - np.min(emotion_trends[emotion]) + 1e-6)\n",
    "            norm_trend = (norm_trend * (wave_y_end - wave_y_start) // 2).astype(int)  # Scale with padding around the center\n",
    "\n",
    "            # Create points for the shaded region\n",
    "            points = []\n",
    "            for j in range(len(norm_trend)):\n",
    "                x = wave_x + j\n",
    "                y = wave_y_center - norm_trend[j]\n",
    "                points.append((x, y))\n",
    "            \n",
    "            # Define the bottom edge of the shaded region with a gap\n",
    "            bottom_edge = min(wave_y_end, wave_y_center + shadow_height)\n",
    "            points.append((wave_x + len(norm_trend) - 1, bottom_edge - gap))\n",
    "            points.append((wave_x, bottom_edge - gap))\n",
    "            points.append((wave_x, wave_y_center - norm_trend[0]))\n",
    "\n",
    "            # Convert points to numpy array\n",
    "            points = np.array(points, np.int32)\n",
    "            points = points.reshape((-1, 1, 2))\n",
    "\n",
    "            # Draw the shaded region in light black (gray-like color)\n",
    "            cv2.fillPoly(dashboard, [points], (222, 218, 218))  # Light shaded black\n",
    "\n",
    "            # Draw the actual waveform in black\n",
    "            for j in range(1, len(norm_trend)):\n",
    "                cv2.line(dashboard, (wave_x + j - 1, wave_y_center - norm_trend[j - 1]), \n",
    "                         (wave_x + j, wave_y_center - norm_trend[j]), (0, 0, 0), 2)  # Black color for waveform\n",
    "\n",
    "    # Combine the video frame and the dashboard\n",
    "    combined_frame = np.hstack((frame, dashboard))\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "\n",
    "# Function to process the video\n",
    "import tempfile\n",
    "\n",
    "def process_video_with_dashboard(video_path, output_path, box_width=500, box_height=500):\n",
    "    # Open video clip using moviepy\n",
    "    clip = VideoFileClip(video_path)\n",
    "\n",
    "    # Extract audio from the clip\n",
    "    audio = clip.audio\n",
    "\n",
    "    # Cache to hold predictions for current second\n",
    "    cached_emotion_probs = np.zeros(len(emotions))\n",
    "    emotion_trends = {emotion: [] for emotion in emotions}  # Reset trends\n",
    "\n",
    "    # VideoWriter to save the processed video\n",
    "    temp_video = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n",
    "    video_writer = None\n",
    "\n",
    "    for t, frame in clip.iter_frames(with_times=True, fps=clip.fps):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert from RGB (moviepy) to BGR (OpenCV)\n",
    "\n",
    "        # Resize the frame to fit into the box\n",
    "        frame_resized = cv2.resize(frame, (box_width, box_height))\n",
    "\n",
    "        # Extract corresponding audio chunk for the current frame\n",
    "        audio_chunk = audio.subclip(t, t + (1 / clip.fps)).to_soundarray()\n",
    "\n",
    "        if len(audio_chunk) > 0:\n",
    "            audio_chunk = audio_chunk.flatten()  # Flatten stereo to mono\n",
    "\n",
    "            # Predict emotion at the current timestamp\n",
    "            emotion_label, emotion_probs = predict_audio_emotion(audio_chunk, audio.fps)\n",
    "            cached_emotion_probs[:] = emotion_probs  # Update cache with new probabilities\n",
    "\n",
    "            # Update emotion trends\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                emotion_trends[emotion].append(emotion_probs[i])\n",
    "                if len(emotion_trends[emotion]) > 300:  # Limit length of trend data to wave_width\n",
    "                    emotion_trends[emotion].pop(0)\n",
    "\n",
    "        # Draw the dashboard on the frame with cached predictions\n",
    "        combined_frame = draw_dashboard(frame_resized, cached_emotion_probs, emotion_trends)\n",
    "\n",
    "        # Initialize the video writer once the first frame is processed\n",
    "        if video_writer is None:\n",
    "            height, width, _ = combined_frame.shape\n",
    "            video_writer = cv2.VideoWriter(temp_video.name, cv2.VideoWriter_fourcc(*'mp4v'), clip.fps, (width, height))\n",
    "\n",
    "        # Write the processed frame to the video file\n",
    "        video_writer.write(combined_frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "\n",
    "    # Combine the processed video with the original audio\n",
    "    final_clip = VideoFileClip(temp_video.name).set_audio(audio)\n",
    "    \n",
    "    # Save the final output video\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "# Example usage\n",
    "process_video_with_dashboard(r\"C:\\Users\\User\\Downloads\\laugh.mp4\", 'output_laugh.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf9ab0e-0787-4672-abd3-a2e7cbec4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "#done and dusted#################################\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Load the trained model and label encoder\n",
    "model = load_model('voice_emotion_recognition.h5')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Define emotions and initialize trends\n",
    "emotions = ['anger', 'calm', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotion_trends = {emotion: [] for emotion in emotions}\n",
    "\n",
    "# Function to extract features from an audio chunk\n",
    "def extract_features(audio_chunk, rate):\n",
    "    mfccs = librosa.feature.mfcc(y=audio_chunk, sr=rate, n_mfcc=40)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs\n",
    "\n",
    "# Function to predict emotion from audio\n",
    "def predict_audio_emotion(audio_chunk, rate):\n",
    "    features = extract_features(audio_chunk, rate)\n",
    "    features = np.reshape(features, (1, -1, 1))\n",
    "    prediction = model.predict(features)\n",
    "    label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "    return label[0], prediction[0]\n",
    "\n",
    "# Function to generate a simulated alpha wave signal (EEG-like waveform)\n",
    "def generate_alpha_wave(length, amplitude=10, frequency=10):\n",
    "    t = np.linspace(0, 1, length)\n",
    "    alpha_wave = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "    return alpha_wave\n",
    "\n",
    "# Function to draw the dashboard with EEG-like waveform\n",
    "def draw_dashboard(frame, emotion_probs, emotion_trends):\n",
    "    # Dashboard settings\n",
    "    dashboard_width = 800\n",
    "    dashboard_height = frame.shape[0]\n",
    "    dashboard = np.ones((dashboard_height, dashboard_width, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "    # Bar settings\n",
    "    bar_height = dashboard_height // 8\n",
    "    bar_width = 300\n",
    "    wave_width = 300\n",
    "    shadow_height = 20  # Height of the shaded region\n",
    "    gap = 10  # Gap between shaded region and the bottom of the rectangle\n",
    "    vertical_padding = 10  # Padding to prevent waves from touching edges\n",
    "\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        # Draw emotion labels\n",
    "        cv2.putText(dashboard, emotion, (10, (i + 1) * bar_height - vertical_padding), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "        # Draw emotion probability bars\n",
    "        cv2.rectangle(dashboard, (110, i * bar_height), (110 + bar_width, (i + 1) * bar_height - vertical_padding), (0, 0, 0), 2)\n",
    "        cv2.rectangle(dashboard, (110, i * bar_height), \n",
    "                      (110 + int(bar_width * emotion_probs[i]), (i + 1) * bar_height - vertical_padding), \n",
    "                      (0, 255, 0), -1)  # Green color for bars\n",
    "\n",
    "        # Draw emotion trend waveforms centered vertically in the box\n",
    "        wave_x = 450\n",
    "        wave_y_start = i * bar_height + vertical_padding\n",
    "        wave_y_end = (i + 1) * bar_height - vertical_padding\n",
    "\n",
    "        # Calculate the center of the rectangle\n",
    "        wave_y_center = (wave_y_start + wave_y_end) // 2\n",
    "\n",
    "        # Draw the rectangle for the waveform\n",
    "        cv2.rectangle(dashboard, (wave_x, i * bar_height), (wave_x + wave_width, (i + 1) * bar_height - vertical_padding), (0, 0, 0), 2)\n",
    "\n",
    "        # Normalize trend data\n",
    "        if len(emotion_trends[emotion]) > 1:\n",
    "            # Add the new data point\n",
    "            emotion_trends[emotion].append(emotion_probs[i])\n",
    "            if len(emotion_trends[emotion]) > wave_width:\n",
    "                emotion_trends[emotion].pop(0)\n",
    "\n",
    "            norm_trend = (np.array(emotion_trends[emotion]) - np.min(emotion_trends[emotion])) / (np.max(emotion_trends[emotion]) - np.min(emotion_trends[emotion]) + 1e-6)\n",
    "            norm_trend = (norm_trend * (wave_y_end - wave_y_start) // 2).astype(int)  # Scale with padding around the center\n",
    "\n",
    "            # Create points for the shaded region\n",
    "            points = []\n",
    "            for j in range(len(norm_trend)):\n",
    "                x = wave_x + j\n",
    "                y = wave_y_center - norm_trend[j]\n",
    "                points.append((x, y))\n",
    "            \n",
    "            # Define the bottom edge of the shaded region with a gap\n",
    "            bottom_edge = min(wave_y_end, wave_y_center + shadow_height)\n",
    "            points.append((wave_x + len(norm_trend) - 1, bottom_edge - gap))\n",
    "            points.append((wave_x, bottom_edge - gap))\n",
    "            points.append((wave_x, wave_y_center - norm_trend[0]))\n",
    "\n",
    "            # Convert points to numpy array\n",
    "            points = np.array(points, np.int32)\n",
    "            points = points.reshape((-1, 1, 2))\n",
    "\n",
    "            # Draw the shaded region in light black (gray-like color)\n",
    "            cv2.fillPoly(dashboard, [points], (222, 218, 218))  # Light shaded black\n",
    "\n",
    "            # Draw the actual waveform in black\n",
    "            for j in range(1, len(norm_trend)):\n",
    "                cv2.line(dashboard, (wave_x + j - 1, wave_y_center - norm_trend[j - 1]), \n",
    "                         (wave_x + j, wave_y_center - norm_trend[j]), (0, 0, 0), 2)  # Black color for waveform\n",
    "\n",
    "    # Combine the video frame and the dashboard\n",
    "    combined_frame = np.hstack((frame, dashboard))\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "\n",
    "# Function to process the video\n",
    "def process_video_with_dashboard(video_path, box_width=500, box_height=500, frame_rate=30):\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the video's frame rate (FPS)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_delay = int(1000 / frame_rate)  # Convert frame rate to milliseconds per frame\n",
    "\n",
    "    # Extract audio using moviepy\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio_rate = audio.fps\n",
    "\n",
    "    # Cache to hold predictions for current second\n",
    "    cached_emotion_probs = np.zeros(len(emotions))\n",
    "\n",
    "    current_frame = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to fit into the box\n",
    "        frame_resized = cv2.resize(frame, (box_width, box_height))\n",
    "\n",
    "        # Determine the time position of the current frame\n",
    "        time_position = current_frame / fps\n",
    "\n",
    "        # Extract corresponding audio chunk for the current frame\n",
    "        audio_chunk = audio.subclip(time_position, time_position + (1/fps)).to_soundarray()\n",
    "\n",
    "        # Predict emotion every N frames (e.g., once per second or so)\n",
    "        if current_frame % int(fps) == 0:  # Predict once per second\n",
    "            audio_chunk = audio_chunk.flatten()  # Flatten stereo to mono\n",
    "            if len(audio_chunk) > 0:\n",
    "                emotion_label, emotion_probs = predict_audio_emotion(audio_chunk, audio_rate)\n",
    "                cached_emotion_probs = emotion_probs  # Update cache with new probabilities\n",
    "\n",
    "                # Update emotion trends\n",
    "                for i, emotion in enumerate(emotions):\n",
    "                    emotion_trends[emotion].append(emotion_probs[i])\n",
    "                    if len(emotion_trends[emotion]) > 300:  # Limit length of trend data to wave_width\n",
    "                        emotion_trends[emotion].pop(0)\n",
    "\n",
    "        # Draw the dashboard on the frame with cached predictions\n",
    "        combined_frame = draw_dashboard(frame_resized, cached_emotion_probs, emotion_trends)\n",
    "\n",
    "        # Display the frame with the dashboard\n",
    "        cv2.imshow('Video with Emotion Detection Dashboard', combined_frame)\n",
    "\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):  # Use frame_delay for synchronized playback\n",
    "            break\n",
    "\n",
    "        current_frame += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage with a video file\n",
    "process_video_with_dashboard('sample1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c1e11-0af9-403e-bb39-d906c2d57846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec699724-2656-4bae-a342-4ca9365e7a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
